{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "import time\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn.datasets\n",
    "\n",
    "import tflib as lib\n",
    "import tflib.save_images\n",
    "import tflib.mnist\n",
    "import tflib.plot\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uppercase local vars:\n",
      "\tBATCH_SIZE: 50\n",
      "\tCRITIC_ITERS: 5\n",
      "\tDIM: 64\n",
      "\tF: <module 'torch.nn.functional' from '/home/kadarsh22/miniconda3/lib/python3.6/site-packages/torch/nn/functional.py'>\n",
      "\tITERS: 200000\n",
      "\tLAMBDA: 10\n",
      "\tOUTPUT_DIM: 784\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    gpu = 0\n",
    "\n",
    "DIM = 64 # Model dimensionality\n",
    "BATCH_SIZE = 50 # Batch size\n",
    "CRITIC_ITERS = 5 # For WGAN and WGAN-GP, number of critic iters per gen iter\n",
    "LAMBDA = 10 # Gradient penalty lambda hyperparameter\n",
    "ITERS = 200000 # How many generator iterations to train for\n",
    "OUTPUT_DIM = 784 # Number of pixels in MNIST (28*28)\n",
    "\n",
    "lib.print_model_settings(locals().copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        preprocess = nn.Sequential(\n",
    "            nn.Linear(128, 4*4*4*DIM),nn.BatchNorm1d(4*4*4*DIM),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        block1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(4*DIM, 2*DIM, 5),nn.BatchNorm2d(2*DIM),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        block2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(2*DIM, DIM, 5),nn.BatchNorm2d(DIM),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        deconv_out = nn.ConvTranspose2d(DIM, 1, 8, stride=2)\n",
    "        \n",
    "\n",
    "        self.block1 = block1\n",
    "        self.block2 = block2\n",
    "        self.deconv_out = deconv_out\n",
    "        self.preprocess = preprocess\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "    def forward(self, input):\n",
    "        output = self.preprocess(input)\n",
    "        output = output.view(-1, 4*DIM, 4, 4)\n",
    "        #print output.size()\n",
    "        output = self.block1(output)\n",
    "        #print output.size()\n",
    "        output = output[:, :, :7, :7]\n",
    "        #print output.size()\n",
    "        output = self.block2(output)\n",
    "        #print output.size()\n",
    "        output = self.deconv_out(output)\n",
    "        output = self.tanh(output)\n",
    "        #print output.size()\n",
    "        return output.view(-1, OUTPUT_DIM)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        main = nn.Sequential(\n",
    "            nn.Conv2d(1, DIM, 5, stride=2, padding=2),\n",
    "            # nn.Linear(OUTPUT_DIM, 4*4*4*DIM),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Conv2d(DIM, 2*DIM, 5, stride=2, padding=2),\n",
    "            nn.BatchNorm2d(2*DIM),\n",
    "            # nn.Linear(4*4*4*DIM, 4*4*4*DIM),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Conv2d(2*DIM, 4*DIM, 5, stride=2, padding=2),\n",
    "            # nn.Linear(4*4*4*DIM, 4*4*4*DIM),\n",
    "            nn.BatchNorm2d(4*DIM),\n",
    "            # nn.Linear(4*4*4*DIM, 4*4*4*DIM),\n",
    "            nn.LeakyReLU(True),\n",
    "            # nn.Linear(4*4*4*DIM, 4*4*4*DIM),\n",
    "            # nn.LeakyReLU(True),\n",
    "            # nn.Linear(4*4*4*DIM, 4*4*4*DIM),\n",
    "            # nn.LeakyReLU(True),\n",
    "        )\n",
    "        self.main = main\n",
    "        self.output = nn.Linear(4*4*4*DIM, 1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = input.view(-1, 1, 28, 28)\n",
    "        out = self.main(input)\n",
    "        out = out.view(-1, 4*4*4*DIM)\n",
    "        out = self.output(out)\n",
    "        return out.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image(frame, netG):\n",
    "    noise = torch.randn(BATCH_SIZE, 128)\n",
    "    if use_cuda:\n",
    "        noise = noise.cuda(gpu)\n",
    "    noisev = autograd.Variable(noise, volatile=True)\n",
    "    samples = netG(noisev)\n",
    "    samples = samples.view(BATCH_SIZE, 28, 28)\n",
    "    # print samples.size()\n",
    "\n",
    "    samples = samples.cpu().data.numpy()\n",
    "\n",
    "    lib.save_images.save_images(\n",
    "        samples,\n",
    "        'results/mnist/samples/samples_{}.png'.format(frame)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset iterator\n",
    "train_gen, dev_gen, test_gen = lib.mnist.load(BATCH_SIZE, BATCH_SIZE)\n",
    "def inf_train_gen():\n",
    "    while True:\n",
    "        for images,targets in train_gen():\n",
    "            yield images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_gradient_penalty(netD, real_data, fake_data):\n",
    "    #print real_data.size()\n",
    "    alpha = torch.rand(BATCH_SIZE, 1)\n",
    "    alpha = alpha.expand(real_data.size())\n",
    "    alpha = alpha.cuda(gpu) if use_cuda else alpha\n",
    "\n",
    "    interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n",
    "\n",
    "    if use_cuda:\n",
    "        interpolates = interpolates.cuda(gpu)\n",
    "    interpolates = autograd.Variable(interpolates, requires_grad=True)\n",
    "\n",
    "    disc_interpolates = netD(interpolates)\n",
    "\n",
    "    gradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n",
    "                              grad_outputs=torch.ones(disc_interpolates.size()).cuda(gpu) if use_cuda else torch.ones(\n",
    "                                  disc_interpolates.size()),\n",
    "                              create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * LAMBDA\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (block1): Sequential(\n",
      "    (0): ConvTranspose2d(256, 128, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "  )\n",
      "  (block2): Sequential(\n",
      "    (0): ConvTranspose2d(128, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "  )\n",
      "  (deconv_out): ConvTranspose2d(64, 1, kernel_size=(8, 8), stride=(2, 2))\n",
      "  (preprocess): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=4096, bias=True)\n",
      "    (1): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "  )\n",
      "  (sigmoid): Sigmoid()\n",
      "  (tanh): Tanh()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "netG = Generator()\n",
    "netD = Discriminator()\n",
    "print(netG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_cuda:\n",
    "    netD = netD.cuda(gpu)\n",
    "    netG = netG.cuda(gpu)\n",
    "\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
    "\n",
    "one = torch.ones([])\n",
    "mone = one * -1\n",
    "if use_cuda:\n",
    "    one = one.cuda(gpu)\n",
    "    mone = mone.cuda(gpu)\n",
    "\n",
    "data = inf_train_gen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kadarsh22/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:28: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/home/kadarsh22/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:74: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0. Validation_Loss: -4.0468001568317415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kadarsh22/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1000. Validation_Loss: -2014.830036010742\n",
      "Iteration: 2000. Validation_Loss: -6074.14125\n",
      "Iteration: 3000. Validation_Loss: -12162.165717773438\n",
      "Iteration: 4000. Validation_Loss: -20286.821484375\n",
      "Iteration: 5000. Validation_Loss: -30441.624033203127\n",
      "Iteration: 6000. Validation_Loss: -42636.28169921875\n",
      "Iteration: 7000. Validation_Loss: -56870.359453125\n",
      "Iteration: 8000. Validation_Loss: -73121.011171875\n",
      "Iteration: 9000. Validation_Loss: -91423.325625\n",
      "Iteration: 10000. Validation_Loss: -111748.00078125\n",
      "Iteration: 11000. Validation_Loss: -134101.76609375\n",
      "Iteration: 12000. Validation_Loss: -158514.27453125\n",
      "Iteration: 13000. Validation_Loss: -184960.16828125\n",
      "Iteration: 14000. Validation_Loss: -213414.247734375\n",
      "Iteration: 15000. Validation_Loss: -243929.59046875\n",
      "Iteration: 16000. Validation_Loss: -276469.38203125\n",
      "Iteration: 17000. Validation_Loss: -311078.89828125\n",
      "Iteration: 18000. Validation_Loss: -347613.7075\n",
      "Iteration: 19000. Validation_Loss: -386383.50484375\n",
      "Iteration: 20000. Validation_Loss: -427098.579375\n",
      "Iteration: 21000. Validation_Loss: -469839.6709375\n",
      "Iteration: 22000. Validation_Loss: -514610.159375\n",
      "Iteration: 23000. Validation_Loss: -561425.5365625\n",
      "Iteration: 24000. Validation_Loss: -610287.25\n",
      "Iteration: 25000. Validation_Loss: -661183.3815625\n",
      "Iteration: 26000. Validation_Loss: -713650.11125\n",
      "Iteration: 27000. Validation_Loss: -768567.9821875\n",
      "Iteration: 28000. Validation_Loss: -825474.331875\n",
      "Iteration: 29000. Validation_Loss: -884512.1578125\n",
      "Iteration: 30000. Validation_Loss: -945533.596875\n",
      "Iteration: 31000. Validation_Loss: -1008526.3525\n",
      "Iteration: 32000. Validation_Loss: -1073427.384375\n",
      "Iteration: 33000. Validation_Loss: -1140056.74\n",
      "Iteration: 34000. Validation_Loss: -1208736.42375\n",
      "Iteration: 35000. Validation_Loss: -1279428.27625\n",
      "Iteration: 36000. Validation_Loss: -1352107.901875\n",
      "Iteration: 37000. Validation_Loss: -1426749.73875\n",
      "Iteration: 38000. Validation_Loss: -1503464.705\n",
      "Iteration: 39000. Validation_Loss: -1582187.445\n",
      "Iteration: 40000. Validation_Loss: -1662859.65375\n",
      "Iteration: 41000. Validation_Loss: -1745579.411875\n",
      "Iteration: 42000. Validation_Loss: -1830276.299375\n",
      "Iteration: 43000. Validation_Loss: -1917037.421875\n",
      "Iteration: 44000. Validation_Loss: -2005740.86125\n",
      "Iteration: 45000. Validation_Loss: -2096405.540625\n",
      "Iteration: 46000. Validation_Loss: -2189078.4225\n",
      "Iteration: 47000. Validation_Loss: -2283490.80125\n",
      "Iteration: 48000. Validation_Loss: -2380243.43125\n",
      "Iteration: 49000. Validation_Loss: -2479188.82375\n",
      "Iteration: 50000. Validation_Loss: -2579989.7225\n",
      "Iteration: 51000. Validation_Loss: -2682705.155\n",
      "Iteration: 52000. Validation_Loss: -2787187.62375\n",
      "Iteration: 53000. Validation_Loss: -2893416.16625\n",
      "Iteration: 54000. Validation_Loss: -3002971.375\n",
      "Iteration: 55000. Validation_Loss: -3113681.01625\n",
      "Iteration: 56000. Validation_Loss: -3226469.76\n",
      "Iteration: 57000. Validation_Loss: -3341301.97625\n",
      "Iteration: 58000. Validation_Loss: -3458036.115\n",
      "Iteration: 59000. Validation_Loss: -3575950.93875\n",
      "Iteration: 60000. Validation_Loss: -3697660.83375\n",
      "Iteration: 61000. Validation_Loss: -3820432.4375\n",
      "Iteration: 62000. Validation_Loss: -3945134.235\n",
      "Iteration: 63000. Validation_Loss: -4071978.27\n",
      "Iteration: 64000. Validation_Loss: -4200038.69\n",
      "Iteration: 65000. Validation_Loss: -4331483.9225\n",
      "Iteration: 66000. Validation_Loss: -4464158.1075\n",
      "Iteration: 67000. Validation_Loss: -4598852.3875\n",
      "Iteration: 68000. Validation_Loss: -4735447.955\n",
      "Iteration: 69000. Validation_Loss: -4874086.0925\n",
      "Iteration: 70000. Validation_Loss: -5014440.935\n",
      "Iteration: 71000. Validation_Loss: -5156929.63\n",
      "Iteration: 72000. Validation_Loss: -5301500.055\n",
      "Iteration: 73000. Validation_Loss: -5448257.6425\n",
      "Iteration: 74000. Validation_Loss: -5597006.94\n",
      "Iteration: 75000. Validation_Loss: -5747542.4675\n",
      "Iteration: 76000. Validation_Loss: -5900153.105\n",
      "Iteration: 77000. Validation_Loss: -6054919.2175\n",
      "Iteration: 78000. Validation_Loss: -6211645.07\n",
      "Iteration: 79000. Validation_Loss: -6370234.52\n",
      "Iteration: 80000. Validation_Loss: -6530835.2075\n",
      "Iteration: 81000. Validation_Loss: -6693129.9225\n",
      "Iteration: 82000. Validation_Loss: -6857731.5875\n",
      "Iteration: 83000. Validation_Loss: -7024312.545\n",
      "Iteration: 84000. Validation_Loss: -7192461.71\n",
      "Iteration: 85000. Validation_Loss: -7363730.51\n",
      "Iteration: 86000. Validation_Loss: -7536374.005\n",
      "Iteration: 87000. Validation_Loss: -7710960.4325\n",
      "Iteration: 88000. Validation_Loss: -7887561.8075\n",
      "Iteration: 89000. Validation_Loss: -8066052.1025\n",
      "Iteration: 90000. Validation_Loss: -8246780.09\n",
      "Iteration: 91000. Validation_Loss: -8429496.1\n",
      "Iteration: 92000. Validation_Loss: -8613950.175\n",
      "Iteration: 93000. Validation_Loss: -8800766.93\n",
      "Iteration: 94000. Validation_Loss: -8989224.895\n",
      "Iteration: 95000. Validation_Loss: -9179955.86\n",
      "Iteration: 96000. Validation_Loss: -9372565.27\n",
      "Iteration: 97000. Validation_Loss: -9567240.825\n",
      "Iteration: 98000. Validation_Loss: -9762678.98\n",
      "Iteration: 99000. Validation_Loss: -9961233.045\n",
      "Iteration: 100000. Validation_Loss: -10163269.62\n",
      "Iteration: 101000. Validation_Loss: -10365887.3\n",
      "Iteration: 102000. Validation_Loss: -10570258.79\n",
      "Iteration: 103000. Validation_Loss: -10777169.625\n",
      "Iteration: 104000. Validation_Loss: -10985786.275\n",
      "Iteration: 105000. Validation_Loss: -11196505.29\n",
      "Iteration: 106000. Validation_Loss: -11409142.91\n",
      "Iteration: 107000. Validation_Loss: -11623739.87\n",
      "Iteration: 108000. Validation_Loss: -11840045.62\n",
      "Iteration: 109000. Validation_Loss: -12058607.465\n",
      "Iteration: 110000. Validation_Loss: -12277151.495\n",
      "Iteration: 111000. Validation_Loss: -12500498.825\n",
      "Iteration: 112000. Validation_Loss: -12724794.13\n",
      "Iteration: 113000. Validation_Loss: -12952781.68\n",
      "Iteration: 114000. Validation_Loss: -13181086.66\n",
      "Iteration: 115000. Validation_Loss: -13410030.225\n",
      "Iteration: 116000. Validation_Loss: -13643802.025\n",
      "Iteration: 117000. Validation_Loss: -13877766.725\n",
      "Iteration: 118000. Validation_Loss: -14115145.345\n",
      "Iteration: 119000. Validation_Loss: -14352747.985\n",
      "Iteration: 120000. Validation_Loss: -14593863.14\n",
      "Iteration: 121000. Validation_Loss: -14836557.53\n",
      "Iteration: 122000. Validation_Loss: -15081233.19\n",
      "Iteration: 123000. Validation_Loss: -15328019.225\n",
      "Iteration: 124000. Validation_Loss: -15574337.885\n",
      "Iteration: 125000. Validation_Loss: -15826979.225\n",
      "Iteration: 126000. Validation_Loss: -16079817.7\n",
      "Iteration: 127000. Validation_Loss: -16334138.575\n",
      "Iteration: 128000. Validation_Loss: -16590658.935\n",
      "Iteration: 129000. Validation_Loss: -16849369.17\n",
      "Iteration: 130000. Validation_Loss: -17108543.35\n",
      "Iteration: 131000. Validation_Loss: -17372123.85\n",
      "Iteration: 132000. Validation_Loss: -17636634.53\n",
      "Iteration: 133000. Validation_Loss: -17902458.35\n",
      "Iteration: 134000. Validation_Loss: -18171136.04\n",
      "Iteration: 135000. Validation_Loss: -18441045.08\n",
      "Iteration: 136000. Validation_Loss: -18712236.76\n",
      "Iteration: 137000. Validation_Loss: -18986914.67\n",
      "Iteration: 138000. Validation_Loss: -19262950.29\n",
      "Iteration: 139000. Validation_Loss: -19541233.59\n",
      "Iteration: 140000. Validation_Loss: -19820555.62\n",
      "Iteration: 141000. Validation_Loss: -20102787.64\n",
      "Iteration: 142000. Validation_Loss: -20387347.36\n",
      "Iteration: 143000. Validation_Loss: -20673469.43\n",
      "Iteration: 144000. Validation_Loss: -20962119.83\n",
      "Iteration: 145000. Validation_Loss: -21252125.71\n",
      "Iteration: 146000. Validation_Loss: -21544524.11\n",
      "Iteration: 147000. Validation_Loss: -21838941.27\n",
      "Iteration: 148000. Validation_Loss: -22135048.62\n",
      "Iteration: 149000. Validation_Loss: -22433093.09\n",
      "Iteration: 150000. Validation_Loss: -22732913.04\n",
      "Iteration: 151000. Validation_Loss: -23035175.86\n",
      "Iteration: 152000. Validation_Loss: -23338699.77\n",
      "Iteration: 153000. Validation_Loss: -23645431.24\n",
      "Iteration: 154000. Validation_Loss: -23947059.45\n",
      "Iteration: 155000. Validation_Loss: -24263254.47\n",
      "Iteration: 156000. Validation_Loss: -24575209.17\n",
      "Iteration: 157000. Validation_Loss: -24888885.93\n",
      "Iteration: 158000. Validation_Loss: -25204566.68\n",
      "Iteration: 159000. Validation_Loss: -25520623.57\n",
      "Iteration: 160000. Validation_Loss: -25841474.24\n",
      "Iteration: 161000. Validation_Loss: -26162083.65\n",
      "Iteration: 162000. Validation_Loss: -26487986.47\n",
      "Iteration: 163000. Validation_Loss: -26814584.31\n",
      "Iteration: 164000. Validation_Loss: -27142024.69\n",
      "Iteration: 165000. Validation_Loss: -27472273.83\n",
      "Iteration: 166000. Validation_Loss: -27802628.35\n",
      "Iteration: 167000. Validation_Loss: -28139133.93\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 168000. Validation_Loss: -28465399.3\n",
      "Iteration: 169000. Validation_Loss: -28733850.13\n",
      "Iteration: 170000. Validation_Loss: -28998013.92\n",
      "Iteration: 171000. Validation_Loss: -29251608.22\n",
      "Iteration: 172000. Validation_Loss: -29454380.53\n",
      "Iteration: 173000. Validation_Loss: -29500669.83\n",
      "Iteration: 174000. Validation_Loss: -29780641.24\n",
      "Iteration: 175000. Validation_Loss: -30043379.84\n",
      "Iteration: 176000. Validation_Loss: -30157988.7\n",
      "Iteration: 177000. Validation_Loss: -30514754.83\n",
      "Iteration: 178000. Validation_Loss: -30742734.32\n",
      "Iteration: 179000. Validation_Loss: -30959887.44\n",
      "Iteration: 180000. Validation_Loss: -31169848.72\n",
      "Iteration: 181000. Validation_Loss: -31476902.72\n",
      "Iteration: 182000. Validation_Loss: -31703561.36\n",
      "Iteration: 183000. Validation_Loss: -31961219.96\n",
      "Iteration: 184000. Validation_Loss: -32264136.82\n",
      "Iteration: 185000. Validation_Loss: -32610422.07\n",
      "Iteration: 186000. Validation_Loss: -32955719.31\n",
      "Iteration: 187000. Validation_Loss: -33308402.32\n",
      "Iteration: 188000. Validation_Loss: -33658989.08\n",
      "Iteration: 189000. Validation_Loss: -34007573.34\n",
      "Iteration: 190000. Validation_Loss: -34346920.82\n",
      "Iteration: 191000. Validation_Loss: -34695401.44\n",
      "Iteration: 192000. Validation_Loss: -35032704.2\n",
      "Iteration: 193000. Validation_Loss: -35422346.38\n",
      "Iteration: 194000. Validation_Loss: -35801724.96\n",
      "Iteration: 195000. Validation_Loss: -36180871.98\n",
      "Iteration: 196000. Validation_Loss: -36561592.3\n",
      "Iteration: 197000. Validation_Loss: -36945031.12\n",
      "Iteration: 198000. Validation_Loss: -37329276.04\n",
      "Iteration: 199000. Validation_Loss: -37720691.52\n",
      "Training finish!... save training results\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(ITERS):\n",
    "    start_time = time.time()\n",
    "    ############################\n",
    "    # (1) Update D network\n",
    "    ###########################\n",
    "    for p in netD.parameters():  # reset requires_grad\n",
    "        p.requires_grad = True  # they are set to False below in netG update\n",
    "\n",
    "    for iter_d in range(CRITIC_ITERS):\n",
    "        _data = data.__next__()\n",
    "        real_data = torch.Tensor(_data)\n",
    "        if use_cuda:\n",
    "            real_data = real_data.cuda(gpu)\n",
    "        real_data_v = autograd.Variable(real_data)\n",
    "\n",
    "        netD.zero_grad()\n",
    "\n",
    "        # train with real\n",
    "        D_real = netD(real_data_v)\n",
    "        D_real = D_real.mean()\n",
    "        # print D_real\n",
    "        D_real.backward(mone)\n",
    "\n",
    "        # train with fake\n",
    "        noise = torch.randn(BATCH_SIZE, 128)\n",
    "        if use_cuda:\n",
    "            noise = noise.cuda(gpu)\n",
    "        noisev = autograd.Variable(noise, volatile=True)  # totally freeze netG\n",
    "        fake = autograd.Variable(netG(noisev).data)\n",
    "        inputv = fake\n",
    "        D_fake = netD(inputv)\n",
    "        D_fake = D_fake.mean()\n",
    "        D_fake.backward(one)\n",
    "\n",
    "        # train with gradient penalty\n",
    "        gradient_penalty = calc_gradient_penalty(netD, real_data_v.data, fake.data)\n",
    "        gradient_penalty.backward()\n",
    "\n",
    "        D_cost = D_fake - D_real + gradient_penalty\n",
    "        Wasserstein_D = D_real - D_fake\n",
    "        optimizerD.step()\n",
    "    ############################\n",
    "    # (2) Update G network\n",
    "    ###########################\n",
    "    for p in netD.parameters():\n",
    "        p.requires_grad = False  # to avoid computation\n",
    "    netG.zero_grad()\n",
    "\n",
    "    noise = torch.randn(BATCH_SIZE, 128)\n",
    "    if use_cuda:\n",
    "        noise = noise.cuda(gpu)\n",
    "    noisev = autograd.Variable(noise)\n",
    "    fake = netG(noisev)\n",
    "    G = netD(fake)\n",
    "    G = G.mean()\n",
    "    G.backward(mone)\n",
    "    G_cost = -G\n",
    "    optimizerG.step()\n",
    "#   print('Iteration: {}.  D_Loss: {}. G_Loss: {}'.format(iteration, D_cost, G_cost))\n",
    "    lib.plot.plot('results/mnist/plots/time_iter', time.time() - start_time)\n",
    "    lib.plot.plot('results/mnist/plots/train disc cost', D_cost.cpu().data.numpy())\n",
    "    lib.plot.plot('results/mnist/plots/train gen cost', G_cost.cpu().data.numpy())\n",
    "    lib.plot.plot('results/mnist/plots/wasserstein distance', Wasserstein_D.cpu().data.numpy())\n",
    "\n",
    "#   Write logs and save samples\n",
    " \n",
    "#   Calculate dev loss and generate samples every 1000 iters\n",
    "    if iteration % 1000 == 0:\n",
    "        dev_disc_costs = []\n",
    "        for images,_ in dev_gen():\n",
    "            imgs = torch.Tensor(images)\n",
    "            if use_cuda:\n",
    "                imgs = imgs.cuda(gpu)\n",
    "            imgs_v = autograd.Variable(imgs, volatile=True)\n",
    "\n",
    "            D = netD(imgs_v)\n",
    "            _dev_disc_cost = -D.mean().cpu().data.numpy()\n",
    "            dev_disc_costs.append(_dev_disc_cost)\n",
    "        dev_loss = sum(dev_disc_costs)/len(dev_disc_costs)\n",
    "        print('Iteration: {}. Validation_Loss: {}'.format(iteration, dev_loss))        \n",
    "        lib.plot.plot('results/mnist/plots/dev disc cost', np.mean(dev_disc_costs))\n",
    "        generate_image(iteration, netG)\n",
    "        lib.plot.flush()\n",
    "        \n",
    "    lib.plot.tick()\n",
    "print(\"Training finish!... save training results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(netG.state_dict(), \"results/mnist/model/generator_param.pkl\")\n",
    "torch.save(netD.state_dict(), \"results/mnist/model/discriminator_param.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
