{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.autograd import Variable\n",
    "from scipy.stats import truncnorm\n",
    "import torch.nn.init as init\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "torch.backends.cudnn.enabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w0_enc = tf.get_variable(\"gaussian_MLP_encoder/w0\", shape=[784, 500])\n",
    "# b0_enc = tf.get_variable(\"gaussian_MLP_encoder/b0\", shape=[500])\n",
    "# w1_enc = tf.get_variable(\"gaussian_MLP_encoder/w1\", shape=[500, 500])\n",
    "# b1_enc = tf.get_variable(\"gaussian_MLP_encoder/b1\", shape=[500])\n",
    "# w2_enc = tf.get_variable(\"gaussian_MLP_encoder/w2\", shape=[500, 40])\n",
    "# b2_enc = tf.get_variable(\"gaussian_MLP_encoder/b2\", shape=[40])\n",
    "\n",
    "\n",
    "# w0_dec = tf.get_variable(\"bernoulli_MLP_decoder/w0\", shape=[20, 500])\n",
    "# b0_dec = tf.get_variable(\"bernoulli_MLP_decoder/b0\", shape=[500])\n",
    "# w1_dec = tf.get_variable(\"bernoulli_MLP_decoder/w1\", shape=[500, 500])\n",
    "# b1_dec = tf.get_variable(\"bernoulli_MLP_decoder/b1\", shape=[500])\n",
    "# w2_dec = tf.get_variable(\"bernoulli_MLP_decoder/w2\", shape=[500, 784])\n",
    "# b2_dec = tf.get_variable(\"bernoulli_MLP_decoder/b2\", shape=[784])\n",
    "\n",
    "# saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.Session() as sess:\n",
    "# #   chkp.print_tensors_in_checkpoint_file(\"/home/iistlab/Desktop/daal_code-master/models/model_class_10_dim_20.ckpt\", tensor_name='bernoulli_MLP_decoder/b0', all_tensors=True)\n",
    "#     saver.restore(sess, \"/home/iistlab/Desktop/daal_code-master/models/model_class_10_dim_20.ckpt\")\n",
    "#     w0_encoder = nn.Parameter(torch.transpose(torch.Tensor(w0_enc.eval()),1,0))\n",
    "#     b0_encoder = nn.Parameter(torch.Tensor(b0_enc.eval()))\n",
    "#     w1_encoder = nn.Parameter(torch.transpose(torch.Tensor(w1_enc.eval()),1,0))\n",
    "#     b1_encoder = nn.Parameter(torch.Tensor(b1_enc.eval()))\n",
    "#     w2_encoder = nn.Parameter(torch.transpose(torch.Tensor(w2_enc.eval()),1,0))\n",
    "#     b2_encoder = nn.Parameter(torch.Tensor(b2_enc.eval()))\n",
    "    \n",
    "#     w0_decoder = nn.Parameter(torch.transpose(torch.Tensor(w0_dec.eval()),1,0))\n",
    "#     b0_decoder = nn.Parameter(torch.Tensor(b0_dec.eval()))\n",
    "#     w1_decoder = nn.Parameter(torch.transpose(torch.Tensor(w1_dec.eval()),1,0))\n",
    "#     b1_decoder = nn.Parameter(torch.Tensor(b1_dec.eval()))\n",
    "#     w2_decoder = nn.Parameter(torch.transpose(torch.Tensor(w2_dec.eval()),1,0))\n",
    "#     b2_decoder = nn.Parameter(torch.Tensor(b2_dec.eval()))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_epochs = 200\n",
    "\n",
    "n_hidden = 500 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.ToTensor()),\n",
    "    batch_size= batch_size, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.ToTensor()),\n",
    "    batch_size= batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def kaiming_init(m):\n",
    "#     if isinstance(m, (nn.Linear)):\n",
    "#         init.kaiming_uniform(m.weight)\n",
    "#         if m.bias is not None:\n",
    "#             m.bias.data.fill_(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, nz):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.have_cuda = False\n",
    "        self.nz = nz\n",
    "\n",
    "        self.fc1 = nn.Linear(784, n_hidden)\n",
    "        self.fc2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.fc3 = nn.Linear(n_hidden, self.nz * 2)\n",
    "        \n",
    "        self.fc1.weight = w0_encoder\n",
    "        self.fc1.bias = b0_encoder\n",
    "        \n",
    "        self.fc2.weight = w1_encoder\n",
    "        self.fc2.bias = b1_encoder\n",
    "            \n",
    "        self.fc3.weight = w2_encoder\n",
    "        self.fc3.bias = b2_encoder\n",
    "        \n",
    "    \n",
    "        self.fc4 = nn.Linear(nz,  n_hidden)\n",
    "        self.fc5 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.fc6 = nn.Linear(n_hidden, 784)\n",
    "            \n",
    "        self.fc4.weight = w0_decoder\n",
    "        self.fc4.bias = b0_decoder\n",
    "        \n",
    "        self.fc5.weight = w1_decoder\n",
    "        self.fc5.bias = b1_decoder\n",
    "        \n",
    "        self.fc6.weight = w2_decoder\n",
    "        self.fc6.bias = b2_decoder\n",
    "        \n",
    "\n",
    "        self.softplus = nn.Softplus()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.elu = nn.ELU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.dropout_encoder = nn.Dropout(0.1)\n",
    "        self.dropout_decoder = nn.Dropout(0.1)\n",
    "        \n",
    "#         self.weight_init()\n",
    "        \n",
    "#     def weight_init(self):\n",
    "#         for block in self._modules:\n",
    "#             kaiming_init(block)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = self.dropout_encoder(self.elu(self.fc1(x)))\n",
    "        h2 = self.dropout_encoder(self.tanh(self.fc2(h1)))\n",
    "        gaussian_params = self.fc3(h2)\n",
    "        mean = gaussian_params[:, :self.nz]\n",
    "        stddev = 10**-6 + self.softplus(gaussian_params[:,self.nz:])\n",
    "        return mean, stddev\n",
    "\n",
    "    def decode(self, z) :\n",
    "        h1 = self.dropout_decoder(self.tanh(self.fc4(z)))\n",
    "        h2 = self.dropout_decoder(self.elu(self.fc5(h1)))\n",
    "        deconv_input = self.sigmoid(self.fc6(h2))\n",
    "        deconv_input = torch.clamp(deconv_input, 10**-8, 1 - 10**-8)\n",
    "        return deconv_input\n",
    "\n",
    "    def reparametrize(self, mu, std):\n",
    "#         std = logvar.mul(0.5).exp()\n",
    "        eps = torch.cuda.FloatTensor(std.size()).normal_()\n",
    "        eps = Variable(eps)\n",
    "        return mu + std*eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1,784)\n",
    "        mu, stddev = self.encode(x)\n",
    "        # print(\"mu, logvar\", mu.size(), logvar.size())\n",
    "        z = self.reparametrize(mu, stddev)\n",
    "        # print(\"z\", z.size())\n",
    "        decoded = self.decode(z)\n",
    "        # print(\"decoded\", decoded.size())\n",
    "        return decoded, mu, stddev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'w0_encoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-92a00d249da7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVAE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdagrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-20acfe2ab792>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, nz)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnz\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw0_encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb0_encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'w0_encoder' is not defined"
     ]
    }
   ],
   "source": [
    "model = VAE(nz=20).to(device)\n",
    "optimizer = optim.Adagrad(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(recon_x, x, mu, sigma):\n",
    "    x  = x.view(-1,784).to(device)\n",
    "    BCE = - torch.sum(x * torch.log(recon_x) + (1-x) * torch.log(1 - recon_x), 1)\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + torch.log(sigma.pow(2)) - mu.pow(2) - sigma.pow(2),1)\n",
    "    return BCE.mean(), KLD.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs) :\n",
    "    train_loss_sum = 0\n",
    "    model.train()\n",
    "    for batch_idx, (data, _) in enumerate(train_loader) :\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        bce, kld = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss = bce + kld\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_sum += loss.data\n",
    "    train_loss = train_loss_sum / len(train_loader)\n",
    "    \n",
    "    test_loss_sum = 0\n",
    "   \n",
    "    for batch_idx, (data, _) in enumerate(test_loader) :\n",
    "        data = data.to(device)\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        bce, kld = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss = bce  + kld\n",
    "        test_loss_sum += loss.data\n",
    "    test_loss = test_loss_sum / len(test_loader)\n",
    "    \n",
    "    print('-----Number of epochs---',epoch) \n",
    "    print('----Training_loss---',train_loss)\n",
    "    print('----Testing loss----',test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.randn(60000,20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
