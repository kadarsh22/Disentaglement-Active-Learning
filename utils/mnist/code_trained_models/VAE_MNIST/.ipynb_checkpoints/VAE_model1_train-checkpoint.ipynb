{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.autograd import Variable\n",
    "from scipy.stats import truncnorm\n",
    "import torch.nn.init as init\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "torch.backends.cudnn.enabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0_enc = tf.get_variable(\"gaussian_MLP_encoder/w0\", shape=[784, 500])\n",
    "b0_enc = tf.get_variable(\"gaussian_MLP_encoder/b0\", shape=[500])\n",
    "w1_enc = tf.get_variable(\"gaussian_MLP_encoder/w1\", shape=[500, 500])\n",
    "b1_enc = tf.get_variable(\"gaussian_MLP_encoder/b1\", shape=[500])\n",
    "w2_enc = tf.get_variable(\"gaussian_MLP_encoder/w2\", shape=[500, 40])\n",
    "b2_enc = tf.get_variable(\"gaussian_MLP_encoder/b2\", shape=[40])\n",
    "\n",
    "\n",
    "w0_dec = tf.get_variable(\"bernoulli_MLP_decoder/w0\", shape=[20, 500])\n",
    "b0_dec = tf.get_variable(\"bernoulli_MLP_decoder/b0\", shape=[500])\n",
    "w1_dec = tf.get_variable(\"bernoulli_MLP_decoder/w1\", shape=[500, 500])\n",
    "b1_dec = tf.get_variable(\"bernoulli_MLP_decoder/b1\", shape=[500])\n",
    "w2_dec = tf.get_variable(\"bernoulli_MLP_decoder/w2\", shape=[500, 784])\n",
    "b2_dec = tf.get_variable(\"bernoulli_MLP_decoder/b2\", shape=[784])\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/iistlab/Desktop/daal_code-master/models/model_class_10_dim_20.ckpt\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "#   chkp.print_tensors_in_checkpoint_file(\"/home/iistlab/Desktop/daal_code-master/models/model_class_10_dim_20.ckpt\", tensor_name='bernoulli_MLP_decoder/b0', all_tensors=True)\n",
    "    saver.restore(sess, \"/home/iistlab/Desktop/daal_code-master/models/model_class_10_dim_20.ckpt\")\n",
    "    w0_encoder = nn.Parameter(torch.transpose(torch.Tensor(w0_enc.eval()),1,0))\n",
    "    b0_encoder = nn.Parameter(torch.Tensor(b0_enc.eval()))\n",
    "    w1_encoder = nn.Parameter(torch.transpose(torch.Tensor(w1_enc.eval()),1,0))\n",
    "    b1_encoder = nn.Parameter(torch.Tensor(b1_enc.eval()))\n",
    "    w2_encoder = nn.Parameter(torch.transpose(torch.Tensor(w2_enc.eval()),1,0))\n",
    "    b2_encoder = nn.Parameter(torch.Tensor(b2_enc.eval()))\n",
    "    \n",
    "    w0_decoder = nn.Parameter(torch.transpose(torch.Tensor(w0_dec.eval()),1,0))\n",
    "    b0_decoder = nn.Parameter(torch.Tensor(b0_dec.eval()))\n",
    "    w1_decoder = nn.Parameter(torch.transpose(torch.Tensor(w1_dec.eval()),1,0))\n",
    "    b1_decoder = nn.Parameter(torch.Tensor(b1_dec.eval()))\n",
    "    w2_decoder = nn.Parameter(torch.transpose(torch.Tensor(w2_dec.eval()),1,0))\n",
    "    b2_decoder = nn.Parameter(torch.Tensor(b2_dec.eval()))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_epochs = 200\n",
    "\n",
    "n_hidden = 500 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.ToTensor()),\n",
    "    batch_size= batch_size, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.ToTensor()),\n",
    "    batch_size= batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def kaiming_init(m):\n",
    "#     if isinstance(m, (nn.Linear)):\n",
    "#         init.kaiming_uniform(m.weight)\n",
    "#         if m.bias is not None:\n",
    "#             m.bias.data.fill_(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, nz):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.have_cuda = False\n",
    "        self.nz = nz\n",
    "\n",
    "        self.fc1 = nn.Linear(784, n_hidden)\n",
    "        self.fc2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.fc3 = nn.Linear(n_hidden, self.nz * 2)\n",
    "        \n",
    "        self.fc1.weight = w0_encoder\n",
    "        self.fc1.bias = b0_encoder\n",
    "        \n",
    "        self.fc2.weight = w1_encoder\n",
    "        self.fc2.bias = b1_encoder\n",
    "            \n",
    "        self.fc3.weight = w2_encoder\n",
    "        self.fc3.bias = b2_encoder\n",
    "        \n",
    "    \n",
    "        self.fc4 = nn.Linear(nz,  n_hidden)\n",
    "        self.fc5 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.fc6 = nn.Linear(n_hidden, 784)\n",
    "            \n",
    "        self.fc4.weight = w0_decoder\n",
    "        self.fc4.bias = b0_decoder\n",
    "        \n",
    "        self.fc5.weight = w1_decoder\n",
    "        self.fc5.bias = b1_decoder\n",
    "        \n",
    "        self.fc6.weight = w2_decoder\n",
    "        self.fc6.bias = b2_decoder\n",
    "        \n",
    "\n",
    "        self.softplus = nn.Softplus()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.elu = nn.ELU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.dropout_encoder = nn.Dropout(0.1)\n",
    "        self.dropout_decoder = nn.Dropout(0.1)\n",
    "        \n",
    "#         self.weight_init()\n",
    "        \n",
    "#     def weight_init(self):\n",
    "#         for block in self._modules:\n",
    "#             kaiming_init(block)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = self.dropout_encoder(self.elu(self.fc1(x)))\n",
    "        h2 = self.dropout_encoder(self.tanh(self.fc2(h1)))\n",
    "        gaussian_params = self.fc3(h2)\n",
    "        mean = gaussian_params[:, :self.nz]\n",
    "        stddev = 10**-6 + self.softplus(gaussian_params[:,self.nz:])\n",
    "        return mean, stddev\n",
    "\n",
    "    def decode(self, z) :\n",
    "        h1 = self.dropout_decoder(self.tanh(self.fc4(z)))\n",
    "        h2 = self.dropout_decoder(self.elu(self.fc5(h1)))\n",
    "        deconv_input = self.sigmoid(self.fc6(h2))\n",
    "        deconv_input = torch.clamp(deconv_input, 10**-8, 1 - 10**-8)\n",
    "        return deconv_input\n",
    "\n",
    "    def reparametrize(self, mu, std):\n",
    "#         std = logvar.mul(0.5).exp()\n",
    "        eps = torch.cuda.FloatTensor(std.size()).normal_()\n",
    "        eps = Variable(eps)\n",
    "        return mu + std*eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1,784)\n",
    "        mu, stddev = self.encode(x)\n",
    "        # print(\"mu, logvar\", mu.size(), logvar.size())\n",
    "        z = self.reparametrize(mu, stddev)\n",
    "        # print(\"z\", z.size())\n",
    "        decoded = self.decode(z)\n",
    "        # print(\"decoded\", decoded.size())\n",
    "        return decoded, mu, stddev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE(nz=20).to(device)\n",
    "optimizer = optim.Adagrad(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(recon_x, x, mu, sigma):\n",
    "    x  = x.view(-1,784).to(device)\n",
    "    BCE = - torch.sum(x * torch.log(recon_x) + (1-x) * torch.log(1 - recon_x), 1)\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + torch.log(sigma.pow(2)) - mu.pow(2) - sigma.pow(2),1)\n",
    "    return BCE.mean(), KLD.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Number of epochs--- 0\n",
      "----Training_loss--- tensor(224.7865, device='cuda:0')\n",
      "----Testing loss---- tensor(188.7895, device='cuda:0')\n",
      "-----Number of epochs--- 1\n",
      "----Training_loss--- tensor(180.9104, device='cuda:0')\n",
      "----Testing loss---- tensor(173.1273, device='cuda:0')\n",
      "-----Number of epochs--- 2\n",
      "----Training_loss--- tensor(170.0452, device='cuda:0')\n",
      "----Testing loss---- tensor(165.3514, device='cuda:0')\n",
      "-----Number of epochs--- 3\n",
      "----Training_loss--- tensor(163.5861, device='cuda:0')\n",
      "----Testing loss---- tensor(160.0003, device='cuda:0')\n",
      "-----Number of epochs--- 4\n",
      "----Training_loss--- tensor(159.0088, device='cuda:0')\n",
      "----Testing loss---- tensor(155.9581, device='cuda:0')\n",
      "-----Number of epochs--- 5\n",
      "----Training_loss--- tensor(155.4852, device='cuda:0')\n",
      "----Testing loss---- tensor(152.8973, device='cuda:0')\n",
      "-----Number of epochs--- 6\n",
      "----Training_loss--- tensor(152.7313, device='cuda:0')\n",
      "----Testing loss---- tensor(150.2906, device='cuda:0')\n",
      "-----Number of epochs--- 7\n",
      "----Training_loss--- tensor(150.5419, device='cuda:0')\n",
      "----Testing loss---- tensor(148.4942, device='cuda:0')\n",
      "-----Number of epochs--- 8\n",
      "----Training_loss--- tensor(148.7972, device='cuda:0')\n",
      "----Testing loss---- tensor(146.8648, device='cuda:0')\n",
      "-----Number of epochs--- 9\n",
      "----Training_loss--- tensor(147.2386, device='cuda:0')\n",
      "----Testing loss---- tensor(145.3944, device='cuda:0')\n",
      "-----Number of epochs--- 10\n",
      "----Training_loss--- tensor(145.9865, device='cuda:0')\n",
      "----Testing loss---- tensor(144.0088, device='cuda:0')\n",
      "-----Number of epochs--- 11\n",
      "----Training_loss--- tensor(144.8339, device='cuda:0')\n",
      "----Testing loss---- tensor(143.2439, device='cuda:0')\n",
      "-----Number of epochs--- 12\n",
      "----Training_loss--- tensor(143.9338, device='cuda:0')\n",
      "----Testing loss---- tensor(142.3195, device='cuda:0')\n",
      "-----Number of epochs--- 13\n",
      "----Training_loss--- tensor(143.0280, device='cuda:0')\n",
      "----Testing loss---- tensor(141.4959, device='cuda:0')\n",
      "-----Number of epochs--- 14\n",
      "----Training_loss--- tensor(142.2606, device='cuda:0')\n",
      "----Testing loss---- tensor(140.5966, device='cuda:0')\n",
      "-----Number of epochs--- 15\n",
      "----Training_loss--- tensor(141.4495, device='cuda:0')\n",
      "----Testing loss---- tensor(139.9876, device='cuda:0')\n",
      "-----Number of epochs--- 16\n",
      "----Training_loss--- tensor(140.8238, device='cuda:0')\n",
      "----Testing loss---- tensor(139.4221, device='cuda:0')\n",
      "-----Number of epochs--- 17\n",
      "----Training_loss--- tensor(140.2156, device='cuda:0')\n",
      "----Testing loss---- tensor(138.6242, device='cuda:0')\n",
      "-----Number of epochs--- 18\n",
      "----Training_loss--- tensor(139.6905, device='cuda:0')\n",
      "----Testing loss---- tensor(138.3418, device='cuda:0')\n",
      "-----Number of epochs--- 19\n",
      "----Training_loss--- tensor(139.1186, device='cuda:0')\n",
      "----Testing loss---- tensor(137.5218, device='cuda:0')\n",
      "-----Number of epochs--- 20\n",
      "----Training_loss--- tensor(138.5728, device='cuda:0')\n",
      "----Testing loss---- tensor(137.3033, device='cuda:0')\n",
      "-----Number of epochs--- 21\n",
      "----Training_loss--- tensor(138.1151, device='cuda:0')\n",
      "----Testing loss---- tensor(136.6167, device='cuda:0')\n",
      "-----Number of epochs--- 22\n",
      "----Training_loss--- tensor(137.6440, device='cuda:0')\n",
      "----Testing loss---- tensor(136.1913, device='cuda:0')\n",
      "-----Number of epochs--- 23\n",
      "----Training_loss--- tensor(137.2502, device='cuda:0')\n",
      "----Testing loss---- tensor(135.9850, device='cuda:0')\n",
      "-----Number of epochs--- 24\n",
      "----Training_loss--- tensor(136.7981, device='cuda:0')\n",
      "----Testing loss---- tensor(135.3997, device='cuda:0')\n",
      "-----Number of epochs--- 25\n",
      "----Training_loss--- tensor(136.3976, device='cuda:0')\n",
      "----Testing loss---- tensor(134.9417, device='cuda:0')\n",
      "-----Number of epochs--- 26\n",
      "----Training_loss--- tensor(136.0634, device='cuda:0')\n",
      "----Testing loss---- tensor(134.7531, device='cuda:0')\n",
      "-----Number of epochs--- 27\n",
      "----Training_loss--- tensor(135.6219, device='cuda:0')\n",
      "----Testing loss---- tensor(134.3212, device='cuda:0')\n",
      "-----Number of epochs--- 28\n",
      "----Training_loss--- tensor(135.3538, device='cuda:0')\n",
      "----Testing loss---- tensor(133.8975, device='cuda:0')\n",
      "-----Number of epochs--- 29\n",
      "----Training_loss--- tensor(135.0166, device='cuda:0')\n",
      "----Testing loss---- tensor(133.7045, device='cuda:0')\n",
      "-----Number of epochs--- 30\n",
      "----Training_loss--- tensor(134.7238, device='cuda:0')\n",
      "----Testing loss---- tensor(133.3456, device='cuda:0')\n",
      "-----Number of epochs--- 31\n",
      "----Training_loss--- tensor(134.4220, device='cuda:0')\n",
      "----Testing loss---- tensor(133.1234, device='cuda:0')\n",
      "-----Number of epochs--- 32\n",
      "----Training_loss--- tensor(134.0935, device='cuda:0')\n",
      "----Testing loss---- tensor(132.7200, device='cuda:0')\n",
      "-----Number of epochs--- 33\n",
      "----Training_loss--- tensor(133.8090, device='cuda:0')\n",
      "----Testing loss---- tensor(132.6493, device='cuda:0')\n",
      "-----Number of epochs--- 34\n",
      "----Training_loss--- tensor(133.5614, device='cuda:0')\n",
      "----Testing loss---- tensor(132.2493, device='cuda:0')\n",
      "-----Number of epochs--- 35\n",
      "----Training_loss--- tensor(133.3136, device='cuda:0')\n",
      "----Testing loss---- tensor(132.0298, device='cuda:0')\n",
      "-----Number of epochs--- 36\n",
      "----Training_loss--- tensor(133.0439, device='cuda:0')\n",
      "----Testing loss---- tensor(131.7863, device='cuda:0')\n",
      "-----Number of epochs--- 37\n",
      "----Training_loss--- tensor(132.8624, device='cuda:0')\n",
      "----Testing loss---- tensor(131.5372, device='cuda:0')\n",
      "-----Number of epochs--- 38\n",
      "----Training_loss--- tensor(132.6309, device='cuda:0')\n",
      "----Testing loss---- tensor(131.4400, device='cuda:0')\n",
      "-----Number of epochs--- 39\n",
      "----Training_loss--- tensor(132.3427, device='cuda:0')\n",
      "----Testing loss---- tensor(131.1170, device='cuda:0')\n",
      "-----Number of epochs--- 40\n",
      "----Training_loss--- tensor(132.1265, device='cuda:0')\n",
      "----Testing loss---- tensor(130.7791, device='cuda:0')\n",
      "-----Number of epochs--- 41\n",
      "----Training_loss--- tensor(132.0154, device='cuda:0')\n",
      "----Testing loss---- tensor(130.8306, device='cuda:0')\n",
      "-----Number of epochs--- 42\n",
      "----Training_loss--- tensor(131.7601, device='cuda:0')\n",
      "----Testing loss---- tensor(130.4961, device='cuda:0')\n",
      "-----Number of epochs--- 43\n",
      "----Training_loss--- tensor(131.5944, device='cuda:0')\n",
      "----Testing loss---- tensor(130.4058, device='cuda:0')\n",
      "-----Number of epochs--- 44\n",
      "----Training_loss--- tensor(131.3835, device='cuda:0')\n",
      "----Testing loss---- tensor(130.2359, device='cuda:0')\n",
      "-----Number of epochs--- 45\n",
      "----Training_loss--- tensor(131.2014, device='cuda:0')\n",
      "----Testing loss---- tensor(129.9569, device='cuda:0')\n",
      "-----Number of epochs--- 46\n",
      "----Training_loss--- tensor(131.0048, device='cuda:0')\n",
      "----Testing loss---- tensor(129.9133, device='cuda:0')\n",
      "-----Number of epochs--- 47\n",
      "----Training_loss--- tensor(130.8326, device='cuda:0')\n",
      "----Testing loss---- tensor(129.5420, device='cuda:0')\n",
      "-----Number of epochs--- 48\n",
      "----Training_loss--- tensor(130.6885, device='cuda:0')\n",
      "----Testing loss---- tensor(129.4443, device='cuda:0')\n",
      "-----Number of epochs--- 49\n",
      "----Training_loss--- tensor(130.5589, device='cuda:0')\n",
      "----Testing loss---- tensor(129.3292, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs) :\n",
    "    train_loss_sum = 0\n",
    "    model.train()\n",
    "    for batch_idx, (data, _) in enumerate(train_loader) :\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        bce, kld = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss = bce + kld\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_sum += loss.data\n",
    "    train_loss = train_loss_sum / len(train_loader)\n",
    "    \n",
    "    test_loss_sum = 0\n",
    "   \n",
    "    for batch_idx, (data, _) in enumerate(test_loader) :\n",
    "        data = data.to(device)\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        bce, kld = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss = bce  + kld\n",
    "        test_loss_sum += loss.data\n",
    "    test_loss = test_loss_sum / len(test_loader)\n",
    "    \n",
    "    print('-----Number of epochs---',epoch) \n",
    "    print('----Training_loss---',train_loss)\n",
    "    print('----Testing loss----',test_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
